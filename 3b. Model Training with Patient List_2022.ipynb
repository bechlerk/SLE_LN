{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training w/ Patient List\n",
    "\n",
    "In this notebook, we'll demonstrate how to generate CLMBR features given a list of patients and labels using a pre-trained CLMBR model, and train a simple classifier on top of a CLMBR-featurized dataset. The example dataset used here is a STARR OMOP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ehr_ml.timeline\n",
    "import ehr_ml.ontology\n",
    "import ehr_ml.index\n",
    "import ehr_ml.labeler\n",
    "import ehr_ml.clmbr\n",
    "import pandas as pd\n",
    "\n",
    "from path_utils import load_extract_paths, update_extract_paths, load_dataset_paths, update_dataset_paths\n",
    "\n",
    "MACHINE = 'nero'\n",
    "DATASET = 'starr_omop_deid'\n",
    "VERSION = '2021_12_13'\n",
    "#paths = load_dataset_paths(MACHINE, DATASET, VERSION)\n",
    "EHR_ML_EXTRACT_DIR = '/home/kbechler/dataset'\n",
    "#paths.get('extract_dir', None)\n",
    "CLMBR_INFO_DIR = \"/invalid\" \n",
    "#paths.get('info_dir', None)\n",
    "MODEL_DIR = '/home/kbechler/model2'\n",
    "#paths.get('model_dir', None)\n",
    "#EXAMPLE_CSV = paths.get('example_csv', None)\n",
    "\n",
    "# make any updates to the paths dictionary, and update the persistent config file with:\n",
    "# update_dataset_paths(MACHINE, DATASET, VERSION, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EHR_ML_EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizing from a list of patients\n",
    "In this use case, we assume that the user has prepared a list of patients, day offsets, and labels for use with training the model. Such a list can come from a query to BigQuery or similar service. We'll load in an example CSV file to demonstrate the minimum spec required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with SLE dataframe of patient list\n",
    "dataframe = pd.read_csv('/home/kbechler/piton_private/df_clmbr_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove patients less than 14 years old\n",
    "more_14 = pd.read_csv('/home/kbechler/piton_private/patient_df_less_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_14 = list(more_14.subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe[dataframe.patient_id.isin(patient_14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataframe.label == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataframe.label == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove held out test set\n",
    "# Import held out test set to remove patients\n",
    "heldout_test_set = pd.read_csv('/home/kbechler/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = list(heldout_test_set.patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_train = dataframe[~dataframe.patient_id.isin(test_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataframe_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_test = dataframe[dataframe.patient_id.isin(test_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataframe_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that if the data was the direct result of a BigQuery query, the patient IDs here don't exactly correspond to the patient IDs used by the `ehr_ml` Timeline object. Furthermore, we need to convert the date strings to date indices to index the last date we should use to featurize each patient. We'll do this preprocessing below using `ehr_ml.clmbr.convert_patient_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_ml_patient_ids, day_indices = ehr_ml.clmbr.convert_patient_data(EHR_ML_EXTRACT_DIR, \n",
    "                                                                    dataframe_train['patient_id'], dataframe_train['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the correct patient IDs and day indices in hand, we can now generate patient features using our pre-trained model. This is done in two steps:\n",
    "1. Load in the pre-trained model with `ehr_ml.clmbr.CLMBRFeaturizer.from_pretrained`\n",
    "2. Featurize with the `featurize_patients` method (**NOTE**: this method expects the _converted_ patient IDs and day indices as arguments!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ehr_ml_patient_ids = np.array(ehr_ml_patient_ids)\n",
    "day_indices = np.array(day_indices)\n",
    "labels = dataframe_train['label'].to_numpy()\n",
    "clmbr_model = ehr_ml.clmbr.CLMBR.from_pretrained(MODEL_DIR)\n",
    "features = clmbr_model.featurize_patients(EHR_ML_EXTRACT_DIR, ehr_ml_patient_ids, day_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors of interest for training our machine learning model are `features` and `labels`, which define our patient feature-matrix and our task-specific labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logistic regression model\n",
    "Next, we'll train a logistic regression model which can perform predictions based off our CLMBR representations. We'll first build a simple dataset out of our features and labels, and then define the model. We'll define a simple train / test split so we can measure the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split patients into train / test cohorts, measure accuracy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 100\n",
    "EARLY_STOPPING_EPOCHS = 5\n",
    "seed = 10\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size = 0.8, random_state = seed)\n",
    "\n",
    "model = LogisticRegressionCV(Cs = 10**(np.linspace(-8,8,20)), scoring = \"roc_auc\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(y_test, y_predict[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = [\"accuracy\", \"balanced_accuracy\", \"roc_auc\", \"average_precision\", \"f1\", \"precision\", \"recall\"]\n",
    "def validation_results(clf, X_val, y_val, results):\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_score = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_val, y_pred)\n",
    "    bal_acc = metrics.balanced_accuracy_score(y_val, y_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_val, y_score)\n",
    "    avg_prec = metrics.average_precision_score(y_val, y_score)\n",
    "    f1 = metrics.f1_score(y_val, y_pred)\n",
    "    precision = metrics.precision_score(y_val, y_pred)\n",
    "    recall = metrics.recall_score(y_val, y_pred)     \n",
    "    results['CLMBR'] = {\"accuracy\": acc,\n",
    "                     \"balanced_accuracy\": bal_acc,\n",
    "                     \"roc_auc\": roc_auc,\n",
    "                     \"average_precision\": avg_prec,\n",
    "                     \"f1\": f1,\n",
    "                     \"precision\": precision,\n",
    "                     \"recall\": recall}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc pr curves\n",
    "def plot_roc_pr_curves(X, y, clf, fig_size=(10, 8)):\n",
    "    fig, ax = plt.subplots(1,1,figsize=fig_size)\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X, y, ax=ax)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.savefig('/home/kbechler/piton_private/plots/model_val_roc.png')\n",
    "    plt.show()\n",
    "\n",
    "    # PR Curve and AP (average precision)\n",
    "    fig, ax = plt.subplots(1,1,figsize=fig_size)\n",
    "    pr_disp = metrics.plot_precision_recall_curve(clf, X, y, ax=ax)\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(\"Precision-Recall (PR) Curve\")\n",
    "    plt.savefig('/home/kbechler/piton_private/plots/model_val_pr.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validation_results(model, X_test, y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient = 'index', columns = ['accuracy', 'balanced_accuracy', \n",
    "                                                                                   'roc_auc', 'average_precision', \n",
    "                                                                                   'f1', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/kbechler/piton_private/plots/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(X_test, y_test, model)\n",
    "#plt.savefig('/home/kbechler/piton_private/plots/model_val_auc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained model\n",
    "\n",
    "Next we'll show how we can use our simple linear classifier to perform predictions on patients in new extracts. To do so, we need to provide a list of patient IDs and day offsets from a dataset extract to featurize them. For our purposes, we'll re-use a small list we got from our labeller, but you can imagine that these patient IDs can come from some other source (e.g. a query to BigQuery; in that case, we would have to perform our conversion to `ehr_ml`-friendly patient IDs and day indices again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_ml_patient_ids, day_indices = ehr_ml.clmbr.convert_patient_data(EHR_ML_EXTRACT_DIR, \n",
    "                                                                    dataframe_test['patient_id'], \n",
    "                                                                    dataframe_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ehr_ml_patient_ids = np.array(ehr_ml_patient_ids)\n",
    "day_indices = np.array(day_indices)\n",
    "labels = dataframe_test['label'].to_numpy()\n",
    "clmbr_model = ehr_ml.clmbr.CLMBR.from_pretrained(MODEL_DIR)\n",
    "features = clmbr_model.featurize_patients(EHR_ML_EXTRACT_DIR, ehr_ml_patient_ids, day_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET_SIZE = 1000\n",
    "\n",
    "# pid_subset = ehr_ml_patient_ids[:SUBSET_SIZE]\n",
    "# day_indices_subset = day_indices[:SUBSET_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll generate the features with `featurize_patients`. Note that while we use the same extract dir in this particular example, you will need to change out the extract dir to test on a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = clmbr_model.featurize_patients(EHR_ML_EXTRACT_DIR, pid_subset, day_indices_subset)\n",
    "# features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(features)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = model.predict_log_proba(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(labels, predictions_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(clf, X_test, y_test, fig_size=(10,8)):\n",
    "    # clf: trained classifier (i.e. after using fit function)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "    # print out stats\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Model accuracy: %.3f\\n\" % accuracy)\n",
    "\n",
    "    # precision, recall, and f1-score is usually reported for class 1 (in binary case)\n",
    "    # recall of positive class (1) = sensitivity\n",
    "    # recall of negative class (0) = specificity\n",
    "    # precision of positive class (1) = PPV\n",
    "    # precision of negative class (0) = NPV\n",
    "    print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm_disp = metrics.plot_confusion_matrix(clf, X_test, y_test)  \n",
    "    plt.savefig('/home/kbechler/piton_private/plots/model_test_cm.png')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    auroc = metrics.roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    fig_roc, ax_roc = plt.subplots(1,1,figsize=fig_size)\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_test, y_test, ax=ax_roc)\n",
    "    ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "    ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "    ax_roc.set_title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.savefig('/home/kbechler/piton_private/plots/model_test_roc.png')\n",
    "    plt.show()\n",
    "\n",
    "    # PR Curve and AP (average precision)\n",
    "    fig, ax_pr = plt.subplots(1,1,figsize=fig_size)\n",
    "    pr_disp = metrics.plot_precision_recall_curve(clf, X_test, y_test, ax=ax_pr)\n",
    "    ax_pr.set_xlabel(\"Recall\")\n",
    "    ax_pr.set_ylabel(\"Precision\")\n",
    "    ax_pr.set_title(\"Precision-Recall (PR) Curve\")\n",
    "    plt.savefig('/home/kbechler/piton_private/plots/model_test_pr.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "test_results = validation_results(model, features, labels, test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame.from_dict(test_results, orient = 'index', columns = ['accuracy', 'balanced_accuracy', \n",
    "                                                                                   'roc_auc', 'average_precision', \n",
    "                                                                                   'f1', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.to_csv('/home/kbechler/piton_private/plots/test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both validation and test roc on same plot\n",
    "\n",
    "def plot_both(clf, X_val, y_val, X_test, y_test, fig_size=(10,8)):\n",
    "    # clf: trained classifier (i.e. after using fit function)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_pred_proba = clf.predict_proba(X_val)\n",
    "\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    auroc = metrics.roc_auc_score(y_val, y_pred_proba[:, 1])\n",
    "    fig_roc, ax_roc = plt.subplots(1,1,figsize=fig_size)\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_val, y_val, ax=ax_roc, name = \"Internal Validation\")\n",
    "    ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "    ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "    ax_roc.set_title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    \n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_proba_test = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    auroc_test = metrics.roc_auc_score(y_test, y_pred_proba_test[:, 1])\n",
    "\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_test, y_test, ax=ax_roc, name = \"Internal Evaluation on Held-Out Test Set\")\n",
    "    plt.savefig('/home/kbechler/piton_private/plots/test_val_roc.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both(model, X_test, y_test, features, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
