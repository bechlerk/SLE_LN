{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and Load Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from pandas import read_csv\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from datetime import date\n",
    "import kaplanmeier as km\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from qqman import qqman\n",
    "from pandas_plink import read_plink\n",
    "from scipy.stats import ttest_ind\n",
    "import math\n",
    "import os\n",
    "from lifelines import KaplanMeierFitter\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import preprocessing\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from xgboost import XGBClassifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Exposure Cohort from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exp_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 'kbechler'\n",
    "nero_gcp_project = 'som-nero-nigam-starr'\n",
    "cdm_project_id = 'som-nero-nigam-starr'\n",
    "cdm_dataset_id = 'starr_omop_cdm5_deid_20211213'\n",
    "work_project_id = 'som-nero-nigam-starr'\n",
    "work_dataset_id = f'{user_id}_explore'\n",
    "cdm_subset_dataset_id = 'cdm_subset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use correct path whether you are local or Nero\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/katelyn/.config/gcloud/legacy_credentials/kbechler@stanford.edu/adc.json'  \n",
    "\n",
    "# Set correct Nero project\n",
    "os.environ['GCLOUD_PROJECT'] = nero_gcp_project\n",
    "client = bigquery.Client(project = work_project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import exposure cohort\n",
    "sql = \"select * from som-nero-nigam-starr.kbechler_explore.exp_2022\"\n",
    "exp_df = client.query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bring in Demographic Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person table\n",
    "person = pd.read_csv('person.tsv', sep = '\\t')\n",
    "print(len(person))\n",
    "person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns to bring in \n",
    "features = ['person_id','_gender_name', 'birth_DATETIME', 'race_concept_id', 'race_source_value', 'ethnicity_concept_id', \n",
    "           'ethnicity_source_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframe\n",
    "person_features = person[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(person_features))\n",
    "person_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of blank race:\", sum(person_features.race_concept_id == 0))\n",
    "print(\"Number of blank ethnicity:\", sum(person_features.ethnicity_concept_id == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_features.birth_DATETIME = pd.to_datetime(person_features.birth_DATETIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge person_features with cohort df\n",
    "patient_df = pd.merge(exp_df, person_features, left_on = 'subject_id', right_on = 'person_id', how = 'inner' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cohort start date to datetime\n",
    "patient_df.cohort_start_date = pd.to_datetime(patient_df.cohort_start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate age at SLE diagnosis\n",
    "patient_df['age_at_diagnosis'] = patient_df.cohort_start_date - patient_df.birth_DATETIME\n",
    "patient_df.age_at_diagnosis = pd.to_numeric(patient_df.age_at_diagnosis.dt.days, downcast = 'integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.age_at_diagnosis = patient_df.age_at_diagnosis/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of birth date column and _gender_name column\n",
    "patient_df = patient_df.rename(columns={'birth_DATETIME': 'birth_date', '_gender_name':'gender'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gender to numerical categorical variable\n",
    "# 1: Female\n",
    "# 2: Male\n",
    "patient_df.gender = np.where(patient_df.gender == 'FEMALE', 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = patient_df[patient_df.age_at_diagnosis >= 14]\n",
    "print(\"Total number of patients:\", len(patient_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Race and Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8515 == Asian\n",
    "# 8527 == White\n",
    "# 2000039205 == Other\n",
    "# 8516 == Black\n",
    "# 2000039212 == Unknown\n",
    "# 8557 == Native Hawaiian or Other Pacific Islander\n",
    "# 2000039210 == Patient Refused\n",
    "# 8657 == American Indian or Alaska Native\n",
    "# 0 == Unknown\n",
    "# 2000039200 == Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df['race_final'] = np.where((patient_df.race_concept_id == 2000039200) | \n",
    "                                     (patient_df.race_concept_id == 0) |\n",
    "                                    (patient_df.race_concept_id == 2000039210)|\n",
    "                                    (patient_df.race_concept_id == 2000039212) |\n",
    "                                    (patient_df.race_concept_id == 2000039205 ), 'Unknown', patient_df.race_concept_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.race_final = np.where(patient_df.race_concept_id == 8515, 'Asian', \n",
    "                                 np.where(patient_df.race_concept_id == 8527, 'White', \n",
    "                                         np.where(patient_df.race_concept_id == 8516, 'Black', \n",
    "                                                 np.where(patient_df.race_concept_id == 8557, \n",
    "                                                         'Native Hawaiian or Other Pacific Islander', \n",
    "                                    np.where(patient_df.race_concept_id == 8657, 'American Indian or Alaska Native',\n",
    "                                            patient_df.race_final)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.race_final.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical \n",
    "# Label encoding\n",
    "patient_df['race_binned'] = np.where(patient_df.race_final == 'Asian', 1, patient_df.race_final)\n",
    "patient_df['race_binned'] = np.where(patient_df.race_final == 'White', 2, patient_df.race_binned)\n",
    "patient_df['race_binned'] = np.where(patient_df.race_final == 'Black', 3, patient_df.race_binned)\n",
    "patient_df['race_binned'] = np.where(patient_df.race_final == 'Native Hawaiian or Other Pacific Islander',\n",
    "                                     4, patient_df.race_binned)\n",
    "patient_df['race_binned'] = np.where(patient_df.race_final == 'American Indian or Alaska Native', \n",
    "                                     5, patient_df.race_binned)\n",
    "patient_df['race_binned'] = np.where(patient_df.race_final == 'Unknown', 0, patient_df.race_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.race_binned.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.drop(columns = ['race_source_value', 'race_concept_id'], axis = 1, inplace  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38003564 == Non-Hispanic/Non-Latino\n",
    "# 0 = Unknown\n",
    "# 38003563 == Hispanic/Latino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df['ethnicity_final'] = np.where(patient_df.ethnicity_concept_id == 38003563, 'Hispanic/Latino', \n",
    "                                        np.where(patient_df.ethnicity_concept_id ==38003564, \n",
    "                                                'Non-Hispanic/Non-Latino', 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df['ethnicity_binned'] = np.where(patient_df.ethnicity_final == 'Unknown', 0, patient_df.ethnicity_final)\n",
    "patient_df.ethnicity_binned = np.where(patient_df.ethnicity_binned == 'Hispanic/Latino',1, patient_df.ethnicity_binned)\n",
    "patient_df.ethnicity_binned = np.where(patient_df.ethnicity_binned == 'Non-Hispanic/Non-Latino', \n",
    "                                       2, patient_df.ethnicity_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.ethnicity_binned.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.drop(columns = ['ethnicity_concept_id', 'ethnicity_source_value'], axis = 1,  inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patient_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize age?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bring in Engineered Feature IDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/1BJ6NkCTlnQZb8VsYwb0ucfLLkQLUuY-1VuzM7tEaecs/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in excel file of curated features from Liya\n",
    "features_file = pd.read_csv('feature_ids_2022.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features_file))\n",
    "features_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bring in Condition Occurrence Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature is count of condition occurrences per patient prior to SLE diagnosis. Could also explore being binary variable of present or not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition occurrence table\n",
    "cond_occ = pd.read_csv('condition_occurrence.tsv', sep='\\t')\n",
    "print(len(cond_occ))\n",
    "cond_occ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to confirm only bringing in conditions prior to cohort start date\n",
    "# Using less than becuase the date includes all medical records of that diagnosis\n",
    "cond_occ = pd.merge(cond_occ, patient_df[['person_id', 'cohort_start_date']], on = 'person_id', how = 'left')\n",
    "cond_occ.condition_start_DATE = pd.to_datetime(cond_occ.condition_start_DATE)\n",
    "cond_occ = cond_occ[cond_occ.condition_start_DATE < cond_occ.cohort_start_date]\n",
    "print(len(cond_occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to appropriate columns\n",
    "features = ['person_id', '_condition_name', 'condition_concept_id']\n",
    "cond_occ_features = cond_occ[features]\n",
    "cond_occ_features['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in condition features from features file\n",
    "condition_features = list(features_file[features_file.table == 'condition'].feature_liya.unique())\n",
    "print(condition_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_occ_features['labels']  = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_concepts = {}\n",
    "def get_concept_ids(feature_list):\n",
    "    for feature in feature_list:\n",
    "        concept_ids = features_file[features_file.feature_liya == feature].concept_id\n",
    "        feature_concepts[feature] = concept_ids\n",
    "        cond_occ_features['labels'] = np.where(cond_occ_features.condition_concept_id.isin(concept_ids), feature,\n",
    "                                              cond_occ_features.labels)\n",
    "get_concept_ids(condition_features)\n",
    "cond_occ_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to patients in cohort\n",
    "cond_occ_features = cond_occ_features[cond_occ_features.person_id.isin(exp_df.subject_id)]\n",
    "print(\"Number of patients with conditions present:\", len(cond_occ_features.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from long to wide dataframe and count condition concept ids\n",
    "# Fill blank values with 0\n",
    "cond_occ_features_wide = cond_occ_features.pivot_table(index = 'person_id', \n",
    "                                                      columns = ['labels'], \n",
    "                                                      values = 'count',\n",
    "                                                      aggfunc = 'any',\n",
    "                                                      fill_value = 0).astype(int).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan column\n",
    "cond_occ_features_wide.drop('nan', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cond_occ_features_wide.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge with patient dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(patient_df, cond_occ_features_wide, left_on = 'subject_id', right_on = 'person_id', how = 'left')\n",
    "df.drop(['person_id_x', 'person_id_y'], axis = 1, inplace = True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bring in Drug Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug table\n",
    "drugs = pd.read_csv('drug_era.tsv', sep = '\\t')\n",
    "print(\"Number of patients in drug table:\", drugs.person_id.nunique())\n",
    "drugs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to drugs prior to cohort start date\n",
    "drugs = pd.merge(drugs, patient_df[['person_id', 'cohort_start_date']], on = 'person_id', how = 'left')\n",
    "drugs.drug_era_start_DATE = pd.to_datetime(drugs.drug_era_start_DATE)\n",
    "drugs = drugs[drugs.drug_era_start_DATE < drugs.cohort_start_date]\n",
    "print(len(drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to appropriate columns\n",
    "features = ['person_id', '_drug_name', 'drug_concept_id', 'drug_exposure_count']\n",
    "drugs_features = drugs[features]\n",
    "drugs_features['labels'] = np.nan\n",
    "\n",
    "# Bring in drugs features from features file\n",
    "drugs_list = list(features_file[(features_file.table == 'drug') & (features_file.include == 'yes')].feature_liya.unique())\n",
    "print(drugs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_concepts = {}\n",
    "def get_concept_ids(feature_list):\n",
    "    for feature in feature_list:\n",
    "        concept_ids = features_file[features_file.feature_liya == feature].concept_id\n",
    "        feature_concepts[feature] = concept_ids\n",
    "        drugs_features['labels'] = np.where(drugs_features.drug_concept_id.isin(concept_ids), feature,\n",
    "                                              drugs_features.labels)\n",
    "\n",
    "get_concept_ids(drugs_list)\n",
    "drugs_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_features.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to patients in cohort\n",
    "drugs_features = drugs_features[drugs_features.person_id.isin(exp_df.subject_id)]\n",
    "print(\"Number of patients with drugs:\", drugs_features.person_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drugs variables present or not for each drug for each patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from long to wide dataframe and count drug concept_ids\n",
    "# Fill blank values with 0\n",
    "drugs_features_wide = drugs_features.pivot_table(index = 'person_id', \n",
    "                                                columns = ['labels'], \n",
    "                                                values = 'drug_exposure_count',\n",
    "                                                aggfunc = 'any', \n",
    "                                                fill_value = 0).astype(int).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_features_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_features_wide.drop('nan', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_features_wide.person_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_features_wide.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge with patient dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(df, drugs_features_wide, left_on = 'subject_id', right_on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_list = list(drugs_features.labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_list.remove('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in NaN with 0 for persons missing drugs from drug exposure table\n",
    "for drug in drug_list:\n",
    "    df1[drug] = df1[drug].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in NaN with 0 for persons missing conditions from condition table\n",
    "cond_list = list(cond_occ_features.labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_list.remove('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond in cond_list:\n",
    "    df1[cond] = df1[cond].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bring in Measurement Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement table\n",
    "measurement = pd.read_csv('measurement.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(measurement))\n",
    "measurement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NA values in measurement values:\", sum(measurement.value_as_number.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NA measurement values\n",
    "measurement = measurement[measurement.value_as_number.notna()]\n",
    "print(\"Number of populated values in measurement table:\", len(measurement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to prior to cohort start date\n",
    "measurement = pd.merge(measurement, patient_df[['person_id', 'cohort_start_date']], on = 'person_id', how = 'left')\n",
    "measurement.measurement_DATE = pd.to_datetime(measurement.measurement_DATE)\n",
    "measurement = measurement[measurement.measurement_DATE < measurement.cohort_start_date]\n",
    "print(\"Number of populated values prior to index date:\", len(measurement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with measurement values:\", measurement.person_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm no null values\n",
    "print(\"Number of null values:\", sum(measurement.value_as_number.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to appropriate columns\n",
    "measurement_features = measurement[['person_id', 'measurement_concept_id', 'value_as_number', '_measurement_name',\n",
    "                                   'measurement_DATETIME', 'unit_source_value']]\n",
    "\n",
    "# List out measurement features \n",
    "measurement_list = list(features_file[features_file.table == 'measurement'].feature_liya.unique())\n",
    "measurement_features['labels'] = np.nan\n",
    "print(measurement_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_concepts = {}\n",
    "def get_concept_ids(feature_list):\n",
    "    for feature in feature_list:\n",
    "        concept_ids = features_file[features_file.feature_liya == feature].concept_id\n",
    "        feature_concepts[feature] = concept_ids\n",
    "        measurement_features['labels'] = np.where(measurement_features.measurement_concept_id.isin(concept_ids), feature,\n",
    "                                              measurement_features.labels)\n",
    "        \n",
    "get_concept_ids(measurement_list)\n",
    "\n",
    "# Remove nan values\n",
    "measurement_features = measurement_features[measurement_features.labels != 'nan']\n",
    "\n",
    "# List out features populated\n",
    "measurement_features.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to patients in cohort\n",
    "measurement_features = measurement_features[measurement_features.person_id.isin(exp_df.subject_id)]\n",
    "print(measurement_features.person_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at units\n",
    "units = measurement_features[['labels','_measurement_name', 'measurement_concept_id', \n",
    "                              'unit_source_value']].drop_duplicates()\n",
    "units = units[units.unit_source_value.notnull()]\n",
    "units.sort_values('labels', ascending = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each person_id, concept_id combination take the row with the max datetime\n",
    "max_date = measurement_features.groupby(['person_id', 'measurement_concept_id', '_measurement_name', 'labels'], \n",
    "                                       as_index = False).agg(\n",
    "{'measurement_DATETIME': max})\n",
    "print(\"Number of unique measurements based on concept id:\", len(max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back with value and labels\n",
    "max_date = pd.merge(max_date, measurement_features[['person_id', 'value_as_number','measurement_concept_id', \n",
    "                    'measurement_DATETIME', 'unit_source_value']], on = ['person_id',  'measurement_concept_id', 'measurement_DATETIME'], \n",
    "                   how = 'left')\n",
    "max_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = max_date.groupby(['person_id', 'measurement_concept_id'], as_index = False).agg({\n",
    "    '_measurement_name':'count'\n",
    "})\n",
    "qc[qc._measurement_name == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date[max_date.person_id == 31419496]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date.unit_source_value = str(max_date.unit_source_value)\n",
    "\n",
    "max_date = max_date.groupby(['person_id', 'measurement_concept_id', '_measurement_name', 'labels',\n",
    "                            'measurement_DATETIME'], as_index = False).agg({\n",
    "    'value_as_number':'mean', \n",
    "    'unit_source_value':lambda x: ' '.join(x)\n",
    "})\n",
    "\n",
    "print(\"Number of patients with measurements:\", len(max_date.person_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore measurement values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Antibody DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary variable. Specific type of ANA antibody present in ~30% of people with SLE. Less than 1% of healthy individuals have antibody."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Present  \n",
    "0: Not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date[max_date.labels == 'anti_double_stranded_DNA_antibody'].value_as_number.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'anti_double_stranded_DNA_antibody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_antibody = max_date[max_date.labels == 'anti_double_stranded_DNA_antibody']\n",
    "dna_antibody = dna_antibody.pivot_table(index = 'person_id', \n",
    "                                                columns = ['labels'], \n",
    "                                                values = 'value_as_number',\n",
    "                                                aggfunc = 'any', \n",
    "                                                fill_value = 0).astype(int).reset_index()\n",
    "dna_antibody.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Units in IU/mL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df1.drop('person_id', inplace = True, axis = 1)\n",
    "df1 = df1.rename(columns = {'subject_id': 'person_id'})\n",
    "df2 = pd.merge(df1, dna_antibody, on = 'person_id', how = 'left')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C3 and C4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low C3 (abnormal < 86 mg/dl, normal => 86 mg/dl, unknown NA) and low C4 (abnormal <= 20 mg/dl, normal > 20mg/dl, unknown NA) levels occur in active lupus. Categorical variable if both or one level is low. Mean of all C3 and C4 measurements taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C3/C4\n",
    "\n",
    "1 Abnormal  \n",
    "0 Normal  \n",
    "-1 NA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3_names = features_file[(features_file.feature_liya == 'c3')].name_starr_omop\n",
    "c4_names = features_file[(features_file.feature_liya == 'c4')].name_starr_omop\n",
    "c3 = max_date[max_date._measurement_name.isin(c3_names)]\n",
    "c4 = max_date[max_date._measurement_name.isin(c4_names)]\n",
    "\n",
    "c3_values = c3.pivot(index = 'person_id', \n",
    "                                 columns = ['_measurement_name'], \n",
    "                                 values = 'value_as_number').reset_index()\n",
    "c3_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with c3 values populated:\", c3_values.person_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can remove column with 'Complement C3a'. Unit source value for C3 is mg/dL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = c3[c3._measurement_name != 'Complement C3a [Mass/volume] in Serum or Plasma']\n",
    "c3_values = c3.pivot_table(index = 'person_id',\n",
    "                          columns = ['_measurement_name'], \n",
    "                          values = 'value_as_number').reset_index()\n",
    "\n",
    "c3_values.describe()\n",
    "c3_values.columns = ['person_id', 'c3_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'c3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abnormal < 86 mg/dl, normal => 86 mg/dl, unknown NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abnormal <= 20 mg/dl, normal > 20mg/dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_values = c4.pivot_table(index = 'person_id', \n",
    "                                 columns = ['_measurement_name'], \n",
    "                                 values = 'value_as_number').reset_index()\n",
    "print(len(c4_values))\n",
    "print(len(c4_values.person_id.unique()))\n",
    "c4_values.columns = ['person_id', 'c4_final']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also remove C4a column. Unit source value is mg/dL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df2 = pd.merge(df2, c3_values, on = 'person_id', how = 'left')\n",
    "df2 = pd.merge(df2, c4_values, on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### erythrocyte_sedimentation_rate feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab test marker for inflammation. Faster than normal rate may indicate a systemic disease. ESR is 0–14 mm/h for males under 50 years old, 0–19 mm/h for females under 50 years old, 0–19 for males between 50 and 80 years old, and 0–29 mm/h for females between 50 and 80 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal  \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore values\n",
    "esr_names = features_file[features_file.feature_liya == 'erythrocyte_sedimentation_rate'].name_starr_omop\n",
    "esr = max_date[max_date._measurement_name.isin(esr_names)]\n",
    "esr_values = esr.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = ['value_as_number', 'measurement_DATETIME']\n",
    "                            ).reset_index()\n",
    "esr_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(esr_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values.columns = esr_values.columns.droplevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can remove esr by Westergren method. Units = mm/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values.drop('Erythrocyte sedimentation rate by Westergren method', axis = 1, inplace = True)\n",
    "esr_values.columns = ['person_id', 'esr_rate', 'esr_rate_measurement', 'esr_rate_date', 'esr_rate_measurement_date']\n",
    "esr_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values['esr_final'] = np.nan\n",
    "esr_values.esr_final = np.where(esr_values.esr_rate_measurement.isna(),esr_values.esr_rate, esr_values.esr_final)\n",
    "print(sum(esr_values.esr_final.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values.esr_final = np.where(esr_values.esr_rate.isna(),\n",
    "                                esr_values.esr_rate_measurement, esr_values.esr_final)\n",
    "print(sum(esr_values.esr_final.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values.esr_rate_date = pd.to_datetime(esr_values.esr_rate_date)\n",
    "esr_values.esr_rate_measurement_date = pd.to_datetime(esr_values.esr_rate_measurement_date)\n",
    "sum(esr_values.esr_final.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values.esr_final = np.where((esr_values.esr_final.isna())&\n",
    "\n",
    "                                (esr_values.esr_rate_date > esr_values.esr_rate_measurement_date),\n",
    "                                esr_values.esr_rate, esr_values.esr_final)\n",
    "esr_values.esr_final = np.where(esr_values.esr_final.isna(), esr_values.esr_rate_measurement, esr_values.esr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'erythrocyte_sedimentation_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values = esr_values[esr_values.esr_final.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esr_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with esr values:\", len(esr_values.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df2 = pd.merge(df2, esr_values[['person_id', 'esr_final']], on = 'person_id',  how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c_reactive feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab test marker for inflammation. abnormal if >= 0.5 mg/dl or 5mg/L, normal if below, NA if unknown. Categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (>5 mg/L, > 0.5 mg/dl)  \n",
    "0: Normal (<5 mg/L, > 0.5 mg/dl)  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'c_reactive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive_names = features_file[features_file.feature_liya == 'c_reactive'].name_starr_omop\n",
    "c_reactive = max_date[max_date._measurement_name.isin(c_reactive_names)]\n",
    "c_reactive_values = c_reactive.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()\n",
    "c_reactive_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive = c_reactive[['person_id', 'measurement_concept_id', '_measurement_name', 'labels', \n",
    "                        'measurement_DATETIME', 'value_as_number']]\n",
    "c_reactive = pd.merge(c_reactive, measurement_features[['person_id', 'measurement_concept_id', 'value_as_number', \n",
    "                                                       'measurement_DATETIME', 'unit_source_value']], \n",
    "                     on = ['person_id', 'measurement_concept_id', 'value_as_number', 'measurement_DATETIME'], \n",
    "                     how = 'left').drop_duplicates()\n",
    "print(len(c_reactive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive[c_reactive.unit_source_value.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive.unit_source_value = np.where(c_reactive.unit_source_value.isnull(), 'mg/dL', c_reactive.unit_source_value)\n",
    "c_reactive.unit_source_value = np.where(c_reactive.unit_source_value == 'mg/dl', 'mg/dL', c_reactive.unit_source_value)\n",
    "c_reactive.unit_source_value = np.where(c_reactive.unit_source_value == 'MG/L', 'mg/L', c_reactive.unit_source_value)\n",
    "c_reactive.unit_source_value = np.where(c_reactive.unit_source_value == 'mg/L (See scan or EMR data for detail)',\n",
    "                                       'mg/L', c_reactive.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = c_reactive.groupby(['unit_source_value'], as_index = False).agg({\n",
    "    'person_id': 'nunique', \n",
    "    'value_as_number': ['mean', 'min', 'max']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mg/L to mg/L\n",
    "c_reactive['value_final'] = np.where(c_reactive.unit_source_value == 'mg/L', c_reactive.value_as_number/10, \n",
    "                                    c_reactive.value_as_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive.value_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one column with data. Some units are mg/L, some are mg/dl. Convert all to mg/dl in 'value_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive_values = c_reactive.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = ['value_final', 'measurement_DATETIME']).reset_index()\n",
    "c_reactive_values.columns = c_reactive_values.columns.droplevel()\n",
    "c_reactive_values.columns = ['person_id', 'c_reactive1', 'c_reactive2', 'c_reactive3', 'c_reactive1_date', \n",
    "                     'c_reactive2_date', 'c_reactive3_date']\n",
    "c_reactive_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive_values['c_reactive_final'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive_values['c_reactive_final'] = np.where((c_reactive_values.c_reactive3.isnull())&(c_reactive_values.c_reactive2.isnull()), \n",
    "                                                 c_reactive_values.c_reactive1, c_reactive_values.c_reactive_final)\n",
    "\n",
    "c_reactive_values.c_reactive_final = np.where((c_reactive_values.c_reactive_final.isnull())&(c_reactive_values.c_reactive2.isnull()), \n",
    "                                             c_reactive_values.c_reactive3, c_reactive_values.c_reactive_final)\n",
    "\n",
    "c_reactive_values.c_reactive_final = np.where(c_reactive_values.c_reactive_final.isnull(), \n",
    "                                              c_reactive_values.c_reactive2, c_reactive_values.c_reactive_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c_reactive_values.c_reactive_final.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(c_reactive_values))\n",
    "print(\"Number of patients with c_reactive values:\", len(c_reactive_values.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df2 = pd.merge(df2, c_reactive_values[['person_id', 'c_reactive_final']], on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creatinine feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic metabolic panel. Clinically, typically can ignore values below 1.0, and focus on continuous scale on values 1.0 and higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_names = features_file[features_file.feature_liya == 'creatinine'].name_starr_omop\n",
    "creatinine = max_date[max_date._measurement_name.isin(creatinine_names)]\n",
    "creatinine_values = creatinine.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creatinine_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'creatinine'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in units for each creatinine measurement\n",
    "creatinine = creatinine[['person_id', 'measurement_concept_id', '_measurement_name', 'labels', 'measurement_DATETIME', \n",
    "                        'value_as_number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine = pd.merge(creatinine, measurement_features[['person_id', 'measurement_concept_id', 'measurement_DATETIME', \n",
    "                                                       'value_as_number', 'unit_source_value']], \n",
    "                     on = ['person_id', 'measurement_concept_id', 'measurement_DATETIME', 'value_as_number'], \n",
    "                     how = 'left').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value = np.where((creatinine.unit_source_value == 'MG/DL')|(creatinine.unit_source_value == 'mg/dl') \n",
    "                                       | (creatinine.unit_source_value == 'mg/dL (See scan or EMR data for detail)' )\n",
    "                                       | (creatinine.unit_source_value == 'MG/DL             '), 'mg/dL', \n",
    "                                       creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine[creatinine.unit_source_value.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.measurement_concept_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at nan by concept_id\n",
    "creatinine[(creatinine.measurement_concept_id == 3016723) &(creatinine.unit_source_value.isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Creatinine [Mass/volume] in Serum or Plasma, 3016723 to mg/dL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value = np.where((creatinine.measurement_concept_id == 3016723) &(creatinine.unit_source_value.isna()), \n",
    "                                       'mg/dL', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at nan by concept_id\n",
    "creatinine[(creatinine.measurement_concept_id == 4013964) &(creatinine.unit_source_value.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all 'Creatinine measurement, serum' to 'mg/dL'\n",
    "creatinine.unit_source_value = np.where((creatinine.measurement_concept_id==4013964) & (creatinine.unit_source_value.isnull()), \n",
    "                                                                                     'mg/dL', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at nan by concept_id\n",
    "creatinine[(creatinine.measurement_concept_id == 4324383) &(creatinine.unit_source_value.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all 'Creatinine measurement, serum' to 'mg/dL'\n",
    "creatinine.unit_source_value = np.where((creatinine.measurement_concept_id==4324383) & (creatinine.unit_source_value.isnull()), \n",
    "                                                                                     'mg/dL', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If value less than 10, make mg/dL\n",
    "creatinine.unit_source_value = np.where((creatinine.value_as_number < 10) & (creatinine.unit_source_value.isnull()), \n",
    "                                                                                     'mg/dL', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value = np.where((creatinine.unit_source_value == 'mg/Day') | (creatinine.unit_source_value\n",
    "                            == 'mg/d'), 'mg/24hr', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Corrected creatinine measurement\n",
    "# Concept ID: 44790124\n",
    "creatinine = creatinine[creatinine.measurement_concept_id != 44790124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = creatinine.groupby(['unit_source_value'], as_index = False).agg({\n",
    "    'value_as_number': ['mean', 'min', 'max'], \n",
    "    'person_id': 'nunique'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine[creatinine.unit_source_value.isnull()]._measurement_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine[creatinine._measurement_name == 'Creatinine [Mass/volume] in Urine'].unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine[creatinine.unit_source_value.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value = np.where((creatinine.unit_source_value.isnull()) & (creatinine._measurement_name == \n",
    "                                'Creatinine [Mass/volume] in Urine'), 'mg/dL', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine[(creatinine._measurement_name == 'Creatinine measurement') & (creatinine.unit_source_value.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value = np.where((creatinine.unit_source_value.isnull() )& (creatinine._measurement_name ==\n",
    "                            'Creatinine measurement'), 'mg/dL', creatinine.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all values with mg/dL as units and take max date of those\n",
    "creatinine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_proper_units = creatinine[creatinine.unit_source_value == 'mg/dL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(creatinine_proper_units))\n",
    "print(len(creatinine_proper_units.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_proper_units.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_proper_units = creatinine_proper_units.sort_values(['person_id', 'measurement_DATETIME'], ascending = False).groupby(['person_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_proper_units.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_proper_units[creatinine_proper_units.value_as_number > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean this remove all rows with value_as_number greater than 10\n",
    "creatinine_proper_units = creatinine_proper_units[creatinine_proper_units.value_as_number < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(creatinine_proper_units))\n",
    "print(len(creatinine_proper_units.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_values = creatinine_proper_units.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_values['creatinine_final'] = creatinine_values['Creatinine [Mass/volume] in Serum or Plasma']\n",
    "\n",
    "creatinine_values.creatinine_final = np.where(creatinine_values.creatinine_final.isnull(), \n",
    "                                creatinine_values['Creatinine [Mass/volume] in Urine'], creatinine_values.creatinine_final)\n",
    "\n",
    "creatinine_values.creatinine_final = np.where(creatinine_values.creatinine_final.isnull(), \n",
    "                            creatinine_values['Creatinine measurement'], creatinine_values.creatinine_final)\n",
    "\n",
    "creatinine_values.creatinine_final = np.where(creatinine_values.creatinine_final.isnull(), \n",
    "                            creatinine_values['Creatinine measurement, serum'], creatinine_values.creatinine_final)\n",
    "\n",
    "creatinine_values.creatinine_final = np.where(creatinine_values.creatinine_final.isnull(), \n",
    "                            creatinine_values['Creatinine; blood'], creatinine_values.creatinine_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatinine_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with creatinine values:\", len(creatinine_values.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df \n",
    "df2 = pd.merge(df2, creatinine_values[['person_id', 'creatinine_final']], on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### urine_pcr feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having protein in urine is red flag. Continuous variable? Need to determine value for NA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<150 mg/24 hr normal, abnormal between 150 and 3000, worse category is greater than 3000. -1 for NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr_names = features_file[features_file.feature_liya == 'urine_PCR'].name_starr_omop\n",
    "urine_pcr = max_date[max_date._measurement_name.isin(urine_pcr_names)]\n",
    "urine_pcr_values = urine_pcr.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()\n",
    "urine_pcr_values.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCR in mg/g approximates Protein excretion in mg/24h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'urine_PCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in units for each measurement\n",
    "urine_pcr.drop('unit_source_value', inplace = True, axis = 1)\n",
    "urine_pcr = pd.merge(urine_pcr, measurement_features[['person_id', 'measurement_concept_id', 'value_as_number', \n",
    "                                                       'measurement_DATETIME', 'unit_source_value']], \n",
    "                     on = ['person_id', 'measurement_concept_id', 'value_as_number', 'measurement_DATETIME'], \n",
    "                     how = 'left')\n",
    "urine_pcr._measurement_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = urine_pcr.groupby(['unit_source_value'], as_index = False).agg({\n",
    "    'value_as_number': ['mean', 'min', 'max'], \n",
    "    'person_id': 'nunique'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr['unit_source_value_final'] = urine_pcr.unit_source_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can assume values less than 10 are in mg/mg and those greater than 10 are in mg/g\n",
    "urine_pcr.unit_source_value_final = np.where(urine_pcr.value_as_number < 10, 'mg/mg', \n",
    "                                      urine_pcr.unit_source_value_final)\n",
    "\n",
    "urine_pcr.unit_source_value_final = np.where(urine_pcr.value_as_number > 50, 'mg/g', \n",
    "                                            urine_pcr.unit_source_value_final)\n",
    "\n",
    "urine_pcr.unit_source_value_final = np.where(urine_pcr.unit_source_value_final == 'mg/g creat', 'mg/g', \n",
    "                                            urine_pcr.unit_source_value_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = urine_pcr.groupby(['unit_source_value_final'], as_index = False).agg({\n",
    "    'value_as_number': ['mean', 'min', 'max'], \n",
    "    'person_id': 'nunique'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to mg/g\n",
    "urine_pcr.value_as_number = np.where(urine_pcr.unit_source_value_final == 'mg/g', urine_pcr.value_as_number/1000, \n",
    "                                    urine_pcr.value_as_number)\n",
    "urine_pcr.value_as_number.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr.unit_source_value_final = 'mg/mg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr._measurement_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(urine_pcr.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr = urine_pcr.sort_values(['person_id', 'measurement_DATETIME'], \n",
    "                                  ascending = False).groupby(['person_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr._measurement_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr_values = urine_pcr.pivot(index = 'person_id', \n",
    "                                  columns = ['_measurement_name'], \n",
    "                                  values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "urine_pcr_values.columns = ['person_id', 'urine_pcr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with urine PCR values:\", len(urine_pcr_values.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df2, urine_pcr_values, on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### albumin feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If low indicates chronic disease or inflammation. Higher values in urine is worse. I think you can make this continuous after 30mg/24 hours. Categories: normal less than 30 mg/24 hours, next category (worse within the category the higher it is) 30-300mg/day, then next category, with higher getting worse, is more than 300mg per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (30-300mg/day)  \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin_names = features_file[features_file.feature_liya == 'albumin'].name_starr_omop\n",
    "albumin = max_date[max_date._measurement_name.isin(albumin_names)]\n",
    "albumin_values = albumin.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()\n",
    "albumin_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'albumin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.drop('unit_source_value', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to bring in individual units for each value for albumin\n",
    "albumin = pd.merge(albumin, measurement_features[['person_id', 'measurement_concept_id', 'value_as_number', \n",
    "                                                       'measurement_DATETIME', 'unit_source_value']], \n",
    "                     on = ['person_id', 'measurement_concept_id', 'value_as_number', 'measurement_DATETIME'], \n",
    "                     how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin[albumin._measurement_name == 'Albumin [Mass/volume] in Serum or Plasma'].unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value = np.where(albumin.unit_source_value == 'G/DL', 'g/dL', albumin.unit_source_value)\n",
    "albumin.unit_source_value = np.where(albumin.unit_source_value == 'g/dl', 'g/dL', albumin.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value = np.where(albumin.unit_source_value == 'MG/DL', 'g/dL', albumin.unit_source_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal range is 3.5 to 5.5 g/dL or 35-55 g/liter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's null and less than 10, make units g/dL\n",
    "albumin.unit_source_value = np.where((albumin.unit_source_value.isnull()) &(albumin.value_as_number < 10), \n",
    "                                    'g/dL', albumin.unit_source_value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin[(albumin._measurement_name == 'Albumin [Mass/volume] in Serum or Plasma')\n",
    "        & (albumin.unit_source_value == 'mg/dL')]\n",
    "\n",
    "albumin.value_as_number = np.where(albumin.unit_source_value == 'mg/dL', albumin.value_as_number/1000, \n",
    "                                  albumin.value_as_number)\n",
    "\n",
    "albumin.unit_source_value = np.where(albumin.unit_source_value == 'mg/dL', 'g/dL', \n",
    "                                    albumin.unit_source_value)\n",
    "\n",
    "albumin.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = albumin.groupby(['unit_source_value'], as_index = False).agg({\n",
    "    'value_as_number': ['mean', 'min', 'max'], \n",
    "    'person_id': 'nunique'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin[albumin.unit_source_value == '%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.value_as_number = np.where(albumin.unit_source_value == '%', albumin.value_as_number/10, \n",
    "                                  albumin.value_as_number)\n",
    "albumin.unit_source_value = np.where(albumin.unit_source_value == '%', 'g/dL', \n",
    "                                    albumin.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin = albumin.sort_values(['person_id', 'measurement_DATETIME'], \n",
    "                             ascending = False).groupby(['person_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with albumin values:\", len(albumin.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin._measurement_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(albumin[albumin._measurement_name == 'Albumin [Mass/volume] in Urine']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin[albumin._measurement_name == 'Albumin [Mass/volume] in Serum or Plasma'].unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin[(albumin._measurement_name == 'Albumin [Mass/volume] in Urine')\n",
    "       & (albumin.unit_source_value == 'ug/mL')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value = np.where(albumin.person_id == 30526420, 'g/dL', albumin.unit_source_value)\n",
    "albumin.unit_source_value = np.where(albumin.person_id == 32253578, 'g/dL', albumin.unit_source_value)\n",
    "albumin.unit_source_value = np.where(albumin.person_id == 30649376, 'g/dL', albumin.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin[albumin.unit_source_value == 'mg/L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide by 10,000?\n",
    "albumin.value_as_number = np.where(albumin.unit_source_value == 'mg/L', albumin.value_as_number/10000, \n",
    "                                  albumin.value_as_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin_values = albumin.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin_values = albumin.pivot(index = 'person_id', \n",
    "                                       columns = ['labels'], \n",
    "                                       values = 'value_as_number').reset_index()\n",
    "albumin_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df2 = pd.merge(df2, albumin_values, on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### urine_ACR feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicates if you have albumin in urine. Albumin is protein normally found in blood. Normal is <30 mg/g and abnormal is anything above. Categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (>=30mg/g)  \n",
    "0: Normal  \n",
    "-1: NA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr_names = features_file[features_file.feature_liya == 'urine_ACR'].name_starr_omop\n",
    "urine_acr = max_date[max_date._measurement_name.isin(urine_acr_names)]\n",
    "urine_acr_values = urine_acr.pivot_table(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'urine_ACR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr.drop('unit_source_value', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr = pd.merge(urine_acr, measurement_features[['person_id', 'measurement_concept_id', 'value_as_number', \n",
    "                                                       'measurement_DATETIME', 'unit_source_value']], \n",
    "                     on = ['person_id', 'measurement_concept_id', 'value_as_number', 'measurement_DATETIME'], \n",
    "                     how = 'left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max date per person\n",
    "urine_acr = urine_acr.sort_values(['person_id', 'measurement_DATETIME'], \n",
    "                             ascending = False).groupby(['person_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with urine_acr values:\", len(urine_acr.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr.unit_source_value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr.unit_source_value = np.where(urine_acr.unit_source_value == 'mg/g creat', 'mg/g', urine_acr.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr[urine_acr.unit_source_value == 'mcg/mg creat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mg/g is the same as mcg/mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all to mg/g\n",
    "urine_acr.unit_source_value = np.where(urine_acr.unit_source_value == 'mcg/mg creat', 'mg/g', \n",
    "                                      urine_acr.unit_source_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr_values = urine_acr.pivot(index = 'person_id', \n",
    "                                       columns = ['labels'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_acr_values.columns = ['person_id', 'urine_acr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can just use albumin/creatinine [ratio] in urine column. and remove mass ratio column since has n = 1. Units of mg/g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df2, urine_acr_values, on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gfr feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kidney's filtration rate; indicates how well kidneys are filtering. >90 is normal, <60 indicates kidney disease. Categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (<60)  \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfr_names = features_file[features_file.feature_liya == 'gfr'].name_starr_omop\n",
    "gfr = max_date[max_date._measurement_name.isin(gfr_names)]\n",
    "gfr_values = gfr.pivot(index = 'person_id', \n",
    "                                       columns = ['_measurement_name'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gfr_values.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to look at values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units[units.labels == 'gfr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like units of mL/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take last measurement for each person\n",
    "# Find the max date per person\n",
    "gfr = gfr.sort_values(['person_id', 'measurement_DATETIME'], \n",
    "                             ascending = False).groupby(['person_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gfr))\n",
    "print(\"Number of patients with grf values:\", len(gfr.person_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfr._measurement_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfr_values = gfr.pivot(index = 'person_id', \n",
    "                                       columns = ['labels'], \n",
    "                                       values = 'value_as_number').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfr_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df2 = pd.merge(df2, gfr_values, on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### systolic blood pressure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measures the force your heart exerts on the walls of your arteries each time it beats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1: Unknown   \n",
    "0: Normal < 120   \n",
    "1: Abnormal between 120 and 130  \n",
    "2: Very abnormal > 130\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_names = features_file[features_file.feature_liya == 'systolic_blood_pressure'].name_starr_omop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = max_date[max_date._measurement_name.isin(bp_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = bp.sort_values(['person_id', 'measurement_DATETIME'], \n",
    "                             ascending = False).groupby(['person_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_final = bp[['person_id', 'value_as_number']]\n",
    "bp_final.columns = ['person_id', 'bp_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patient df\n",
    "df2 = pd.merge(df2, bp_final, on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bin measurement features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore measurement values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_values(feature_list, df):\n",
    "    for col in feature_list:\n",
    "        print(\"Column name: %s\" % col)\n",
    "        print(\"Number of NaN values: %d\" % df[col].isna().sum())\n",
    "        print(\"Number of Patients with these values: %d\" % df[col].notna().sum())\n",
    "        print(\"Imputation value (mean): %.3f\" % df[col].mean())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### anti_double_stranded_DNA_antibody\n",
    "\n",
    "Binary: Specific type of ANA antibody present in ~30% of people with SLE. Less than 1% of healthy individuals have antibody. \n",
    "\n",
    "1: Present  \n",
    "0: Not Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2['DNA_antibody_binary'] = np.where(df2.anti_double_stranded_DNA_antibody.isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['anti_double_stranded_DNA_antibody'], df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C3 values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low c3 levels occur in active lupus. Categorical variable if low.   \n",
    "abnormal < 86 mg/dl, normal => 86 mg/dl, unknown NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal < 86 mg/dL   \n",
    "0: Normal >= 86 mg/dL   \n",
    "-1: Unknown NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['c3_binned'] = np.where(df2['c3_final'] < 86, 1,\n",
    "                                  np.where(df2['c3_final']>= 86, \n",
    "                                           0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['c3_final', 'c4_final'], df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C4 values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low c4 levels occur in active lupus. Categorical variable if low.   \n",
    "abnormal < 20 mg/dl, normal => 20mg/dl, unknown NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal < 20 mg/dl    \n",
    "0: Normal => 20 mg/dL  \n",
    "-1: Unknown if NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['c4_binned'] = np.where(df2['c4_final'] < 20, 1,\n",
    "                                  np.where(df2['c4_final']>= 86, \n",
    "                                           0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### erythrocyte_sedimentation_rate feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab test marker for inflammation. Faster than normal rate may indicate a systemic disease. ESR is 0–14 mm/h for males under 50 years old, 0–19 mm/h for females under 50 years old, 0–19 for males between 50 and 80 years old, and 0–29 mm/h for females between 50 and 80 years old. Categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (see above)  \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['esr_final'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['esr_binned'] = np.where((df2.gender == 1) & (df2.esr_final >= 29) & (df2.age_at_diagnosis>50), \n",
    "                            0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.esr_binned = np.where((df2.gender == 1) & (df2.esr_final >=19) & (df2.age_at_diagnosis <= 50), \n",
    "                         0, df2.esr_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.esr_binned = np.where((df2.gender == 2) & (df2.esr_final >=19) & (df2.age_at_diagnosis > 50), \n",
    "                         0, df2.esr_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.esr_binned = np.where((df2.gender == 2) & (df2.esr_final >= 14) & (df2.age_at_diagnosis <= 50), \n",
    "                         0, df2.esr_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df2.esr_binned == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df2.esr_binned == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df2.esr_final.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.esr_binned = np.where(df2.esr_final.isnull(), -1, df2.esr_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.esr_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c_reactive feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab test marker for inflammation. abnormal if >= 0.5 mg/dl or 5mg/L, normal if below, NA if unknown. Categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (>= 0.5mg/dl or 5mg/L)  \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['c_reactive_final'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_reactive_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['c_reactive_binned'] = np.where(df2['c_reactive_final'] > 0.5, 1,\n",
    "                                    np.where(df2.c_reactive_final <= 0.5, 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creatinine feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic metabolic panel. Can ignore values below 1.0, continuous?\n",
    "Categorical: abnormal 1 if greater than 1.0, normal if less, and -1 if NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['creatinine_final'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['creatinine_binned'] = np.where(df2.creatinine_final.isna(), -1,\n",
    "                            np.where(df2.creatinine_final <= 1.0, 0, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.creatinine_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### urine_pcr feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having protein in urine is red flag. I think this can be continuous when higher than 150mg/24 hr. normal<150mg per 24 hour period (makes no difference if they are below that number). Next category (higher in the category is worse) 150-3000, then next category is 3000+ (higher within that category is worse). I think you can just make this continuous after 150mg.\n",
    "\n",
    "\n",
    "I think you can make this continuous after 0.2 mg/mg. less than 0.2 mg/mg is normal (makes no difference if they are below that number), then continuous measure within 0.2-3 for the next highest risk (the higher within the category, the worse), then >3 as the next category of even more risk, with higher values being worse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Worse abnormal (>3)  \n",
    "1: Abnormal (0.2-3)  \n",
    "0: Normal (<0.2)  \n",
    "-1: Unknown if NA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['urine_pcr'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_pcr_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['urine_pcr_binned'] = np.where(df2.urine_pcr >3.0, 2, np.where(df2.urine_pcr > 0.2, 1, \n",
    "                        np.where(df2.urine_pcr <= 0.2, 0, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.urine_pcr_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### albumin feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If low indicates chronic disease or inflammation. The normal range is 3.5 to 5.5 g/dL or 35-55 g/liter. https://loinc.org/1751-7/. Categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal   \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['albumin'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumin_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['albumin_binned'] = np.where((df2.albumin >= 3.5) & (df2.albumin <= 5.5), \n",
    "                                 0, np.where(df2.albumin.isnull(), -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.albumin_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### urine_ACR feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicates if you have albumin in urine. Albumin is protein normally found in blood. Categories: normal less than 30mg/g, 30-300 next worse, 300+ worse. I think you can just make this a continuous variable after the 30mg/g mark. This applies to the urine albumin measurements below as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Worse abnormal (300+)  \n",
    "1: Abnormal (>=30mg/g)  \n",
    "0: Normal  \n",
    "-1: NA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['urine_acr'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['urine_acr_binned'] = np.where(df2.urine_acr <= 30,0, \n",
    "                        np.where(df2.urine_acr.isna(),-1, \n",
    "                        np.where(df2.urine_acr > 300, 2, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.urine_acr_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gfr feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kidney's filtration rate; indicates how well kidneys are filtering. < 60 abnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Abnormal (<60)  \n",
    "0: Normal  \n",
    "-1: NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['gfr'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['gfr_binned'] = np.where(df2.gfr <60, 1, np.where(df2.gfr.isnull(), -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.gfr_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### systolic blood pressure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measures the force your heart exerts on the walls of your arteries each time it beats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1: Unknown   \n",
    "0: Normal < 120   \n",
    "1: Abnormal between 120 and 130  \n",
    "2: Very abnormal > 130\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_values(['bp_final'], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['bp_binned'] = np.where(df2.bp_final <= 120, 0, np.where(df2.bp_final.isnull(), -1, \n",
    "                np.where(df2.bp_final > 130, 2, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.bp_binned.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. NLP features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in Note_nlp features to indicate presence of condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Condition occurrence table\n",
    "# nlp = pd.read_csv('/home/kbechler/notebooks/data/note_nlp.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(nlp))\n",
    "# nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to confirm only bringing in conditions prior to cohort start date\n",
    "# Using less than becuase the date includes all medical records of that diagnosis\n",
    "# nlp = pd.merge(nlp, patient_df[['person_id', 'cohort_start_date']], on = 'person_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subset cond_occ df\n",
    "# nlp = nlp[nlp.condition_start_DATE < nlp.cohort_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subset to appropriate columns\n",
    "# features = ['person_id', '_note_nlp_name', 'note_nlp_concept_id']\n",
    "# nlp_features = nlp[features]\n",
    "# nlp_features['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bring in condition features from features file\n",
    "# nlp_features = list(features_file[features_file.table == 'note_nlp'].feature_liya.unique())\n",
    "# print(nlp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_features['labels']  = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_concepts = {}\n",
    "# def get_concept_ids(feature_list):\n",
    "#     for feature in feature_list:\n",
    "#         concept_ids = features_file[features_file.feature_liya == feature].concept_id\n",
    "#         feature_concepts[feature] = concept_ids\n",
    "#         nlp_features['labels'] = np.where(nlp_features.condition_concept_id.isin(concept_ids), feature,\n",
    "#                                               nlp_features.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_concept_ids(nlp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nlp_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subset to patients in cohort\n",
    "# cond_occ_features = cond_occ_features[cond_occ_features.person_id.isin(exp_df.subject_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cond_occ_features.person_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform from long to wide dataframe and count condition concept ids\n",
    "# # Fill blank values with 0\n",
    "# cond_occ_features_wide = cond_occ_features.pivot_table(index = 'person_id', \n",
    "#                                                       columns = ['labels'], \n",
    "#                                                       values = 'count',\n",
    "#                                                       aggfunc = 'any',\n",
    "#                                                       fill_value = 0).astype(int).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cond_occ_features_wide.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop nan column\n",
    "# cond_occ_features_wide.drop('nan', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Explore features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot distributions of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_type = {'count': ['gender', 'anemia', 'proteinuria', 'synovitis','diabetes', 'hypercholesterolemia', \n",
    "                       'hypertension','azathioprine', 'cyclophosphamide', \n",
    "                       'dexamethasone', 'hydroxychloroquine', 'methylprednisolone', 'mofetil', \n",
    "                      'prednisone', 'rituximab', 'tacrolimus', 'anti_double_stranded_DNA_antibody', \n",
    "                      'race_final', 'ethnicity_final'], \n",
    "            'binned': ['c3', 'c4', 'esr', 'c_reactive', 'creatinine' 'urine_pcr', 'albumin', \n",
    "                      'urine_acr', 'gfr', 'bp'], \n",
    "            'violin': ['age_at_diagnosis']} \n",
    "            #'none': ['race_concept_id', 'ethnicity_concept_id']}\n",
    "fig_size = (20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(plot_type, df3):\n",
    "    \n",
    "    for col in list(df3.columns):\n",
    "#         if col in plot_type['none']:\n",
    "#             continue\n",
    "        \n",
    "        title = \"Distribution of %s column values\" % col\n",
    "        if col in plot_type['count']:\n",
    "            print_value_counts(df3, col)\n",
    "            plt.figure(figsize = fig_size)\n",
    "            sns.countplot(data = df3, x = col).set_title(title)\n",
    "        elif col in plot_type['violin']:\n",
    "            print_value_counts(df3, col)\n",
    "            plt.figure(figsize = fig_size)\n",
    "            sns.violinplot(data = df3, x = col).set_title(title)\n",
    "        elif col in plot_type['binned']:\n",
    "            bin_col = col + \"_binned\"\n",
    "            print_value_counts(df3, bin_col)\n",
    "            fig,axes = plt.subplots(1,2,figsize = fig_size)\n",
    "            fig.suptitle(title)\n",
    "            axes[0].set_title(\"Original Data\")\n",
    "            sns.violinplot(ax = axes[0], data = df3, x = col)\n",
    "            axes[1].set_title(\"Binned Data\")\n",
    "            sns.violinplot(ax = axes[1], data = df3, x = bin_col)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value_counts(df3, col):\n",
    "    print(\"Counts table for column %s\" % col)\n",
    "    print(df2[col].value_counts().sort_index())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distributions(plot_type, df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Finalize dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant/duplicate features\n",
    "df3.drop(['cohort_definition_id', 'birth_date', \n",
    "          'albumin', 'c3_final', 'c4_final', 'c_reactive_final', 'creatinine_final', 'esr_final', \n",
    "         'gfr', 'urine_acr', 'urine_pcr', 'anti_double_stranded_DNA_antibody', 'bp_final',\n",
    "         'cohort_end_date', 'cohort_start_date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in labels\n",
    "labels = pd.read_csv('patient_df_labels_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df3, labels[['subject_id', 'ln_5years']], left_on = 'person_id', right_on = 'subject_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients less than 14:\", sum(df3.age_at_diagnosis <14))\n",
    "print(\"Number of patients greater than 14:\", sum(df3.age_at_diagnosis >=14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of those patient have outcome\n",
    "print(\"Number of patients less than 14 with LN:\", sum((df3.age_at_diagnosis <14) &(df3.ln_5years ==1)))\n",
    "print(\"Number of patients greater than 14 with LN:\", sum((df3.age_at_diagnosis >=14) &(df3.ln_5years ==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Create test train split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove held out test set\n",
    "test_set = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_patients = list(test_set.patient_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(held_out_patients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4[['gender']] = df4[['gender']].replace([1, 2], ['female', 'male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(['race_binned', 'ethnicity_binned'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4[['anemia', 'proteinuria', 'synovitis', 'azathioprine','cyclophosphamide', 'dexamethasone', 'hydroxychloroquine',\n",
    "#     'methylprednisolone', 'mycophenolate mofetil', 'prednisone', 'rituximab', 'tacrolimus', \n",
    "#      'DNA_antibody_binary']] = df4[['anemia', 'proteinuria', 'synovitis', 'azathioprine','cyclophosphamide', \n",
    "#     'dexamethasone', 'hydroxychloroquine','methylprednisolone', 'mycophenolate mofetil', 'prednisone', 'rituximab', \n",
    "#             'tacrolimus', 'DNA_antibody_binary']].replace([0,1], ['not_present', 'present'])\n",
    "\n",
    "df4[['c3_binned', 'c4_binned', 'esr_binned', 'c_reactive_binned', 'creatinine_binned', 'albumin_binned', \n",
    "     'gfr_binned', 'bp_binned']] = df4[['c3_binned', 'c4_binned', 'esr_binned', 'c_reactive_binned', 'creatinine_binned', \n",
    "         'albumin_binned', 'gfr_binned', 'bp_binned']].replace([-1, 0, 1], ['unknown', 'normal', 'abnormal'])\n",
    "\n",
    "df4[['urine_pcr_binned', 'urine_acr_binned']] = df4[['urine_pcr_binned', 'urine_acr_binned']].replace([-1, 0, 1, 2], \n",
    "                    ['unknown', 'normal', 'abnormal', 'v_abnormal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(['person_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one hot encode\n",
    "df_one_hot = df4\n",
    "cat_columns = ['race_final', 'ethnicity_final','c3_binned', 'c4_binned', 'esr_binned',\n",
    "       'c_reactive_binned', 'creatinine_binned', 'urine_pcr_binned',\n",
    "       'albumin_binned', 'urine_acr_binned', 'gfr_binned', 'bp_binned']\n",
    "for col in cat_columns:\n",
    "    col_one_hot = pd.get_dummies(df4[col], prefix = col)\n",
    "    df_one_hot = pd.concat((df_one_hot, col_one_hot), axis = 1).drop(col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_one_hot[~df_one_hot.subject_id.isin(held_out_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(['subject_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_one_hot))\n",
    "print(df_one_hot.head())\n",
    "print(df_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed and validation and train percentages\n",
    "val = 0.25\n",
    "train = 0.75\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_final['ln_5years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.loc[:, df_final.columns != 'ln_5years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, test_size = val, \n",
    "                                                          random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Model Development and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "all_models = {}\n",
    "k = 5\n",
    "# Metrics to score\n",
    "metric = ['accuracy', 'balanced_accuracy', 'roc_auc', 'average_precision', 'f1', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [(\"Logistic Regression Model\", LogisticRegression()),\n",
    "          (\"Logistic Regression CV Model\", LogisticRegressionCV()), \n",
    "          (\"Gradient Boosting Model\", GradientBoostingClassifier()),\n",
    "          (\"Random Forest Model\",  RandomForestClassifier())]\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    scores = model_selection.cross_validate(model, X_train, y_train, scoring=metric, cv=kfold)\n",
    "    results.append((name, scores))\n",
    "\n",
    "fig_size = (10, 5)\n",
    "for name, scores in results:\n",
    "    scores_df = pd.DataFrame.from_dict(scores).drop(columns=[\"fit_time\", \"score_time\"])\n",
    "    scores_df.columns = [col[5:] for col in scores_df.columns]\n",
    "    scores_df = pd.melt(scores_df, var_name=\"Metric\", value_name=\"Score\")\n",
    "    plt.figure(figsize=fig_size)\n",
    "    ax = sns.swarmplot(data=scores_df, x=\"Metric\", y=\"Score\")\n",
    "    ax.set_title(name)\n",
    "    plt.show()\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/models_plots.png')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_results(clf, X_val, y_val, name, results):\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_score = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_val, y_pred)\n",
    "    bal_acc = metrics.balanced_accuracy_score(y_val, y_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_val, y_score)\n",
    "    avg_prec = metrics.average_precision_score(y_val, y_score)\n",
    "    f1 = metrics.f1_score(y_val, y_pred)\n",
    "    precision = metrics.precision_score(y_val, y_pred)\n",
    "    recall = metrics.recall_score(y_val, y_pred)     \n",
    "    results[name] = {\"accuracy\": acc,\n",
    "                     \"balanced_accuracy\": bal_acc,\n",
    "                     \"roc_auc\": roc_auc,\n",
    "                     \"average_precision\": avg_prec,\n",
    "                     \"f1\": f1,\n",
    "                     \"precision\": precision,\n",
    "                     \"recall\": recall}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "penalties = [\"l1\"]\n",
    "C_values = [100, 10, 1.0, 0.1, 0.01] # smaller values = stronger regularization\n",
    "\n",
    "i = 0\n",
    "results = {}\n",
    "models = []\n",
    "for solver, penalty, C in it.product(solvers, penalties, C_values):\n",
    "    if solver in [\"newton-cg\", \"sag\", \"lbfgs\"] and penalty != \"l2\":\n",
    "        # skip parameter configurations not supported by scikit-learn\n",
    "        continue\n",
    "        \n",
    "    print(\"Model #{i} Hyperparameters => Solver: {s}, Penalty: {p}, C: {c}\".format(i=i, s=solver, p=penalty, c=C))\n",
    "    if solver in [\"sag\", \"saga\", \"liblinear\"]:\n",
    "        # these solvers first shuffle the data, so include random seed\n",
    "        model = LogisticRegression(solver=solver, penalty=penalty, random_state=seed)\n",
    "    else:\n",
    "        model = LogisticRegression(solver=solver, penalty=penalty, random_state = seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    results = validation_results(model, X_val, y_val, \"Model #%d\" % i, results)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = [\"lbfgs\"]\n",
    "penalties = [\"l2\"]\n",
    "\n",
    "i = 10\n",
    "for solver, penalty in it.product(solvers, penalties):\n",
    "    if solver in [\"newton-cg\", \"sag\", \"lbfgs\"] and penalty != \"l2\":\n",
    "        # skip parameter configurations not supported by scikit-learn\n",
    "        continue\n",
    "        \n",
    "    print(\"Model #{i} Hyperparameters => Solver: {s}, Penalty: {p},\".format(i=i, s=solver, p=penalty))\n",
    "    if solver in [\"sag\", \"saga\", \"liblinear\"]:\n",
    "        # these solvers first shuffle the data, so include random seed\n",
    "        model = LogisticRegressionCV(solver=solver, cv = 5, penalty=penalty, random_state=seed)\n",
    "    else:\n",
    "        model = LogisticRegressionCV(solver=solver, cv = 5, penalty=penalty, random_state = seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    results = validation_results(model, X_val, y_val, \"Model #%d\" % i, results)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_results(results, model_name, fig_size=(10, 8)):\n",
    "    results_df = pd.DataFrame()\n",
    "    for name, result in results.items():\n",
    "        results_df = results_df.append(result, ignore_index=True)\n",
    "\n",
    "    results_df = pd.melt(results_df, var_name=\"Metric\", value_name=\"Score\")\n",
    "    results_df[\"Name\"] = list(results.keys()) * len(metric)\n",
    "\n",
    "    ncol = (len(results.keys()) // 20) + 1\n",
    "    plt.figure(figsize=fig_size)\n",
    "    ax = sns.swarmplot(data=results_df, x=\"Metric\", y=\"Score\", hue=\"Name\")\n",
    "    ax.set_title(\"Performance of%s Models with Varied Hyperparameters\" % model_name)\n",
    "    plt.legend(bbox_to_anchor=(1.5, 1), loc=\"upper right\", ncol=ncol)\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/'+ str(model_name)+ '_validation_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_results(results, \" Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_validation_results(results, metric_name, threshold):\n",
    "    filtered_models = []\n",
    "    for name, result in results.items():\n",
    "        if result[metric_name] > threshold:\n",
    "            print(name)\n",
    "            filtered_models.append(name)\n",
    "            \n",
    "    return filtered_models\n",
    "\n",
    "def find_models(filtered_results):\n",
    "    for model_name in filtered_results:\n",
    "        print(model_name)\n",
    "        print(results[model_name])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = filter_validation_results(results, \"roc_auc\", 0.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = filter_validation_results(results, \"f1\", 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_models(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_pr_curves(name, X_val, y_val, clf, fig_size=(10, 8)):\n",
    "    fig, ax = plt.subplots(1,1,figsize=fig_size)\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_val, y_val, ax=ax)\n",
    "    ax.set_xlabel(\"False Positive Rate\", fontsize = 18)\n",
    "    ax.set_ylabel(\"True Positive Rate\", fontsize = 18)\n",
    "    ax.set_title(\"Receiver Operating Characteristic (ROC) Curve\", fontsize = 18)\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/' + str(name) + '_val_roc.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # PR Curve and AP (average precision)\n",
    "    fig, ax = plt.subplots(1,1,figsize=fig_size)\n",
    "    pr_disp = metrics.plot_precision_recall_curve(clf, X_val, y_val, ax=ax)\n",
    "    ax.set_xlabel(\"Recall\", fontsize = 18)\n",
    "    ax.set_ylabel(\"Precision\", fontsize = 18)\n",
    "    ax.set_title(\"Precision-Recall (PR) Curve\", fontsize = 18)\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2' + str(name) + '_val_pr.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = 10\n",
    "lr_cv_model = models[cv_model]\n",
    "plot_roc_pr_curves(\"Logistic Regression CV\", X_val, y_val, lr_cv_model)\n",
    "all_results = validation_results(lr_cv_model, X_val, y_val, \"Logistic Regression CV\", all_results)\n",
    "all_models['Logistic Regression CV'] = lr_cv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model_num = 4\n",
    "lr_best_model = models[best_model_num]\n",
    "plot_roc_pr_curves(\"Logistic Regression L1\", X_val, y_val, lr_best_model)\n",
    "all_results = validation_results(lr_best_model, X_val, y_val, \"Logistic Regression L1\", all_results)\n",
    "all_models[\"Logistic Regression L1\"] = lr_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Models: Gradient Boosting and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "gb_baseline = GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 100, max_depth = 3, min_samples_split = 2, \n",
    "                                     min_samples_leaf = 1, subsample = 1, max_features = \"sqrt\", random_state = seed)\n",
    "gb_baseline.fit(X_train, y_train)\n",
    "\n",
    "all_results = validation_results(gb_baseline, X_val, y_val, \"Gradient Boosting Basline\", all_results)\n",
    "all_models[\"Gradient Boosting Baseline\"] = gb_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_1 = {'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1]}\n",
    "param_2 = {'learning_rate':  [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1], 'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750]}\n",
    "param_3 = {'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1], 'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750], 'max_depth': [2, 3, 4]}\n",
    "param_4 = {'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1], \n",
    "              'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750], \n",
    "             'max_depth': [2, 3, 4], \n",
    "             'max_features': [3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_1\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, max_features= 'sqrt',\n",
    "                                                                             n_estimators = 100,\n",
    "                                                             max_depth = 3, min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_1, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_2\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, max_features= 'sqrt',\n",
    "                                                             max_depth = 3, min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_2, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_3\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, max_features= 'sqrt',\n",
    "                                                             min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_3, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_4\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, \n",
    "                                                             min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_4, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_param1 = GradientBoostingClassifier(learning_rate = 0.001, random_state = seed)\n",
    "gb_model_param1.fit(X_train, y_train)\n",
    "predictions = gb_model_param1.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb_model_param2 = GradientBoostingClassifier(learning_rate = 0.005,n_estimators = 500, random_state = seed)\n",
    "gb_model_param2.fit(X_train, y_train)\n",
    "predictions = gb_model_param2.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_param3 = GradientBoostingClassifier(learning_rate = 0.001, n_estimators = 1500, max_depth = 4, random_state = seed)\n",
    "gb_model_param3.fit(X_train, y_train)\n",
    "predictions = gb_model_param3.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_param4 = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 3, max_features = 5,\n",
    "                                             n_estimators = 250, random_state = seed)\n",
    "gb_model_param4.fit(X_train, y_train)\n",
    "predictions = gb_model_param4.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best_model = gb_model_param3\n",
    "all_results = validation_results(gb_best_model, X_val, y_val, \"GB best model\", all_results)\n",
    "all_models[\"GB best model\"] = gb_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(\"Gradient Boosting Baseline\", X_val, y_val, gb_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = seed)\n",
    "\n",
    "# Look at parameters in rf \n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_baseline = RandomForestClassifier(random_state = seed)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "\n",
    "all_results = validation_results(rf_baseline, X_val, y_val, \"Random Forest Baseline\", all_results)\n",
    "all_models[\"Random Forest Baseline\"] = rf_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search grid\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree \n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node \n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random grid\n",
    "random_grid = {'n_estimators': n_estimators, \n",
    "              'max_features': max_features, \n",
    "              'max_depth': max_depth, \n",
    "              'min_samples_split': min_samples_split, \n",
    "              'min_samples_leaf': min_samples_leaf, \n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search training\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state = seed)\n",
    "# Random search of parameters, using 3 fold cross validation\n",
    "# Search across 100 different combinations, and use all available cores\n",
    "rf_random = model_selection.RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, random_state = seed, \n",
    "                              n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random_model = rf_random.best_estimator_\n",
    "\n",
    "all_results = validation_results(rf_random_model, X_val, y_val, \"Random Forest (Best Random Search)\", all_results)\n",
    "all_models[\"Random Forest (Best Random Search)\"] = rf_random_model\n",
    "\n",
    "# View best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(\"Random Forest (Best Random Search)\", X_val, y_val, rf_random_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap':[True],\n",
    "    'max_depth': [9, 10, 11], \n",
    "    'max_features': [3, 4, 5, 6],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'min_samples_split': [1, 2, 3, 4], \n",
    "    'n_estimators': [400, 500, 600, 700, 800, 900]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state = seed)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = model_selection.GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit grid to search data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_grid_model = grid_search.best_estimator_\n",
    "\n",
    "all_results = validation_results(rf_grid_model, X_val, y_val, \"RF best model (Grid Search)\", all_results)\n",
    "all_models[\"Rf best model (Grid Search)\"] = rf_grid_model\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame.from_dict(all_results, orient='index', columns = ['accuracy', 'balanced_accuracy',\n",
    "                                                                               'roc_auc', 'average_precision', \n",
    "                                                                               'f1', 'precision', 'recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_validation_results(all_results, \"All Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(\"Logistic Regression CV\", X_val, y_val, lr_cv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Addressing Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_models = {}\n",
    "os_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "#svc_model = SVC(class_weight='balanced', probability=True)\n",
    "#svc_model.fit(X_train, y_train)\n",
    "# svc_predict = svc_model.predict(X_val)# check performance\n",
    "# print('ROCAUC score:',metrics.roc_auc_score(y_val, svc_predict))\n",
    "# print('Accuracy score:',metrics.accuracy_score(y_val, svc_predict))\n",
    "# print('F1 score:',metrics.f1_score(y_val, svc_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance Weighting, Manual Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression L1 Penalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pen_os = LogisticRegressionCV(solver='liblinear', random_state = seed, cv = 5, penalty = 'l1')\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= lr_pen_os, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(weigh_data['weight'], weigh_data['score'])\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weight of max F1 score\n",
    "value = max(weigh_data.score)\n",
    "weigh_data[weigh_data.score == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing and training the model\n",
    "\n",
    "lr_pen_os = LogisticRegressionCV(solver='liblinear', penalty = 'l1', cv = 5, random_state = seed, \n",
    "                                 class_weight={0: 1-0.53, 1: 0.53})\n",
    "lr_pen_os.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_val = lr_pen_os.predict(X_val)\n",
    "\n",
    "#Calculating and printing the f1 score \n",
    "\n",
    "print('ROCAUC score:',metrics.roc_auc_score(y_val, pred_val))\n",
    "print('Accuracy score:',metrics.accuracy_score(y_val, pred_val))\n",
    "print('F1 score:',metrics.f1_score(y_val, pred_val))\n",
    "\n",
    "os_results = validation_results(lr_pen_os, X_val, y_val, \"Logistic Regression L1 (Importance Weighting)\",\n",
    "                                os_results )\n",
    "os_models['Logistic Regression L1 (Importance Weighting)'] = lr_pen_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "lr_cv_os = LogisticRegressionCV(solver='lbfgs', random_state = seed, cv = 5)\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= lr_cv_os, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(weigh_data['weight'], weigh_data['score'])\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weight of max F1 score\n",
    "value = max(weigh_data.score)\n",
    "weigh_data[weigh_data.score == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing and training the model\n",
    "\n",
    "lr_cv_os = LogisticRegressionCV(solver='lbfgs', cv = 5, random_state = seed, class_weight={0: 1-0.940302, 1: 0.940302})\n",
    "lr_cv_os.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_val = lr_cv_os.predict(X_val)\n",
    "\n",
    "#Calculating and printing the f1 score \n",
    "\n",
    "print('ROCAUC score:',metrics.roc_auc_score(y_val, pred_val))\n",
    "print('Accuracy score:',metrics.accuracy_score(y_val, pred_val))\n",
    "print('F1 score:',metrics.f1_score(y_val, pred_val))\n",
    "\n",
    "os_results = validation_results(lr_cv_os, X_val, y_val, \"Logistic Regression CV (Importance Weighting)\",\n",
    "                                os_results )\n",
    "os_models['Logistic Regression CV (Importance Weighting)'] = lr_cv_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "rf_os = RandomForestClassifier(random_state = seed)\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= rf_os, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(weigh_data['weight'], weigh_data['score'])\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weight of max F1 score\n",
    "value = max(weigh_data.score)\n",
    "weigh_data[weigh_data.score == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing and training the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "rf_os = RandomForestClassifier(random_state = seed, class_weight={0: 1-0.99, 1: 0.99})\n",
    "rf_os.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_val = rf_os.predict(X_val)\n",
    "\n",
    "#Calculating and printing the f1 score \n",
    "\n",
    "print('ROCAUC score:',metrics.roc_auc_score(y_val, pred_val))\n",
    "print('Accuracy score:',metrics.accuracy_score(y_val, pred_val))\n",
    "print('F1 score:',metrics.f1_score(y_val, pred_val))\n",
    "\n",
    "os_results = validation_results(rf_os, X_val, y_val, \"Random Forest (Importance Weighting)\",\n",
    "                                os_results )\n",
    "os_models['Random Forest (Importance Weighting)'] = rf_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE for class imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_val))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(y_train)\n",
    "print(\"before:\", counter)\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "n_features=59, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=seed, k_neighbors = 3)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "counter = Counter(y_res)\n",
    "print(\"after:\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Random Forest Model with balanced dataset using SMOTE\n",
    "rf_rebalanced = RandomForestClassifier(random_state = seed)\n",
    "rf_rebalanced.fit(X_res, y_res)\n",
    "os_results = validation_results(rf_rebalanced, X_val, y_val, \"Random Forest (SMOTE)\",os_results )\n",
    "os_models['Random Forest (SMOTE)'] = rf_rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rebalanced = LogisticRegression(random_state = seed)\n",
    "lr_rebalanced.fit(X_res, y_res)\n",
    "os_results = validation_results(lr_rebalanced, X_val, y_val, \"Logistic Regression (SMOTE)\", os_results)\n",
    "os_models['Logistic Regression (SMOTE)'] = lr_rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_rebalanced = GradientBoostingClassifier(random_state = seed)\n",
    "gb_rebalanced.fit(X_res,y_res)\n",
    "os_results = validation_results(gb_rebalanced, X_val, y_val, \"Gradient Boosting (SMOTE)\", os_results)\n",
    "os_models['Gradient Boosting (SMOTE)'] = gb_rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_res\n",
    "y_train = y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TomekLinks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# tl = TomekLinks()\n",
    "# X_resampled, y_resampled = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(random_state = seed)\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "# test = validation_results(clf, X_val, y_val, \"TomekLinks\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# cc = ClusterCentroids(random_state=seed)\n",
    "# X_resampled, y_resampled = cc.fit_resample(X_train, y_train)\n",
    "\n",
    "# y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(random_state = seed)\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "# test = validation_results(clf, X_val, y_val, \"ClusterCentroids\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use near miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import NearMiss \n",
    "# nr = NearMiss() \n",
    "# X_near, y_near= nr.fit_resample(X_train, y_train.ravel()) \n",
    "# c=Counter(y_near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# lr.fit(X_near, y_near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_near, y_near)\n",
    "# test = validation_results(rf, X_val, y_val, \"RF near miss\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to score\n",
    "metric = ['accuracy', 'balanced_accuracy', 'roc_auc', 'average_precision', 'f1', 'precision', 'recall']\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [(\"Logistic Regression Model\", LogisticRegression()),\n",
    "          (\"Logistic Regression CV Model\", LogisticRegressionCV()), \n",
    "          (\"Gradient Boosting Model\", GradientBoostingClassifier()),\n",
    "          (\"Random Forest Model\",  RandomForestClassifier())]\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    scores = model_selection.cross_validate(model, X_train, y_train, scoring=metric, cv=kfold)\n",
    "    results.append((name, scores))\n",
    "\n",
    "fig_size = (10, 5)\n",
    "for name, scores in results:\n",
    "    scores_df = pd.DataFrame.from_dict(scores).drop(columns=[\"fit_time\", \"score_time\"])\n",
    "    scores_df.columns = [col[5:] for col in scores_df.columns]\n",
    "    scores_df = pd.melt(scores_df, var_name=\"Metric\", value_name=\"Score\")\n",
    "    plt.figure(figsize=fig_size)\n",
    "    ax = sns.swarmplot(data=scores_df, x=\"Metric\", y=\"Score\")\n",
    "    ax.set_title(name)\n",
    "    plt.show()\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/models_plots.png')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "penalties = [\"l1\"]\n",
    "C_values = [100, 10, 1.0, 0.1, 0.01] # smaller values = stronger regularization\n",
    "\n",
    "results = {}\n",
    "models = []\n",
    "i = 0\n",
    "\n",
    "for solver, penalty, C in it.product(solvers, penalties, C_values):\n",
    "    if solver in [\"newton-cg\", \"sag\", \"lbfgs\"] and penalty != \"l2\":\n",
    "        # skip parameter configurations not supported by scikit-learn\n",
    "        continue\n",
    "        \n",
    "    print(\"Model #{i} Hyperparameters => Solver: {s}, Penalty: {p}, C: {c}\".format(i=i, s=solver, p=penalty, c=C))\n",
    "    if solver in [\"sag\", \"saga\", \"liblinear\"]:\n",
    "        # these solvers first shuffle the data, so include random seed\n",
    "        model = LogisticRegression(solver=solver, penalty=penalty, random_state=seed)\n",
    "    else:\n",
    "        model = LogisticRegression(solver=solver, penalty=penalty, random_state = seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    results = validation_results(model, X_val, y_val, \"Model #%d\" % i, results)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = [\"lbfgs\"]\n",
    "penalties = [\"l2\"]\n",
    "\n",
    "i = 10\n",
    "for solver, penalty in it.product(solvers, penalties):\n",
    "    if solver in [\"newton-cg\", \"sag\", \"lbfgs\"] and penalty != \"l2\":\n",
    "        # skip parameter configurations not supported by scikit-learn\n",
    "        continue\n",
    "        \n",
    "    print(\"Model #{i} Hyperparameters => Solver: {s}, Penalty: {p},\".format(i=i, s=solver, p=penalty))\n",
    "    if solver in [\"sag\", \"saga\", \"liblinear\"]:\n",
    "        # these solvers first shuffle the data, so include random seed\n",
    "        model = LogisticRegressionCV(solver=solver, penalty=penalty, random_state=seed)\n",
    "    else:\n",
    "        model = LogisticRegressionCV(solver=solver, penalty=penalty, random_state = seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    results = validation_results(model, X_val, y_val, \"Model #%d\" % i, results)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_results(results, \"Logistic Regression SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = filter_validation_results(results, \"f1\", 0.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = filter_validation_results(results, \"roc_auc\", 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_models(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model_num = 0\n",
    "lr_smote = models[best_model_num]\n",
    "plot_roc_pr_curves(\"Logistic Regression (SMOTE)\", X_val, y_val, lr_smote)\n",
    "os_results = validation_results(lr_smote, X_val, y_val, \"Logistic Regression (SMOTE)\", os_results)\n",
    "os_models[\"Logistic Regression (SMOTE)\"] = lr_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_num = 10\n",
    "lr_cv_smote = models[best_model_num]\n",
    "plot_roc_pr_curves(\"Logistic Regression CV (SMOTE)\", X_val, y_val, lr_cv_smote)\n",
    "os_results = validation_results(lr_cv_smote, X_val, y_val, \"Logistic Regression CV (SMOTE)\", os_results)\n",
    "os_models[\"Logistic Regression CV (SMOTE)\"] = lr_cv_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Models: Gradient Boosting and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "gb_os_baseline = GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 100, max_depth = 3, min_samples_split = 2, \n",
    "                                     min_samples_leaf = 1, subsample = 1, max_features = \"sqrt\", random_state = seed)\n",
    "gb_os_baseline.fit(X_train, y_train)\n",
    "\n",
    "os_results = validation_results(gb_os_baseline, X_val, y_val, \"Gradient Boosting (SMOTE)\", os_results)\n",
    "os_models[\"Gradient Boosting (SMOTE)\"] = gb_os_baseline\n",
    "plot_roc_pr_curves(\"Gradient Boosting (SMOTE)\", X_val, y_val, gb_os_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_1 = {'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1]}\n",
    "param_2 = {'learning_rate':  [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1], 'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750]}\n",
    "param_3 = {'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1], 'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750], 'max_depth': [2, 3, 4]}\n",
    "param_4 = {'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, .25, .5, .75, 1], \n",
    "              'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500, 1750], \n",
    "           \n",
    "           'max_depth': [2, 3, 4], \n",
    "             'max_features': [3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_1\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, max_features= 'sqrt',\n",
    "                                                                             n_estimators = 100,\n",
    "                                                             max_depth = 3, min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_1, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_2\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, max_features= 'sqrt',\n",
    "                                                             max_depth = 3, min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_2, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_3\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, max_features= 'sqrt',\n",
    "                                                             min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_3, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM with param_4\n",
    "tuning = model_selection.GridSearchCV(estimator = GradientBoostingClassifier(subsample = 1, \n",
    "                                                             min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                                             random_state = seed), \n",
    "                     param_grid = param_4, scoring = 'accuracy', n_jobs = 4, cv = 5)\n",
    "\n",
    "tuning.fit(X_train, y_train)\n",
    "tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_param1 = GradientBoostingClassifier(learning_rate = 0.001, random_state = seed)\n",
    "gb_model_param1.fit(X_train, y_train)\n",
    "predictions = gb_model_param1.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb_model_param2 = GradientBoostingClassifier(learning_rate = 0.005,n_estimators = 500, random_state = seed)\n",
    "gb_model_param2.fit(X_train, y_train)\n",
    "predictions = gb_model_param2.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_param3 = GradientBoostingClassifier(learning_rate = 0.001, n_estimators = 1500, max_depth = 4, random_state = seed)\n",
    "gb_model_param3.fit(X_train, y_train)\n",
    "predictions = gb_model_param3.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_param4 = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 3, max_features = 5,\n",
    "                                             n_estimators = 250, random_state = seed)\n",
    "gb_model_param4.fit(X_train, y_train)\n",
    "predictions = gb_model_param4.predict(X_val)\n",
    "print(metrics.classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_os_best_model = gb_model_param2\n",
    "os_results = validation_results(gb_os_best_model, X_val, y_val, \"Gradient Boosting Best (SMOTE)\", os_results)\n",
    "os_models[\"Gradient Boosting Best (SMOTE)\"] = gb_os_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(\"Gradient Boosting Best (SMOTE)\", X_val, y_val, gb_os_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = seed)\n",
    "\n",
    "# Look at parameters in rf \n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_os_baseline = RandomForestClassifier(random_state = seed)\n",
    "rf_os_baseline.fit(X_train, y_train)\n",
    "\n",
    "os_results = validation_results(rf_os_baseline, X_val, y_val, \"Random Forest (SMOTE)\", os_results)\n",
    "os_models[\"Random Forest (SMOTE)\"] = rf_os_baseline\n",
    "plot_roc_pr_curves(\"Random Forest (SMOTE)\", X_val, y_val, rf_os_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search grid\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree \n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node \n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random grid\n",
    "random_grid = {'n_estimators': n_estimators, \n",
    "              'max_features': max_features, \n",
    "              'max_depth': max_depth, \n",
    "              'min_samples_split': min_samples_split, \n",
    "              'min_samples_leaf': min_samples_leaf, \n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search training\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state = seed)\n",
    "# Random search of parameters, using 3 fold cross validation\n",
    "# Search across 100 different combinations, and use all available cores\n",
    "rf_random = model_selection.RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, random_state = seed, \n",
    "                              n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_os_random_model = rf_random.best_estimator_\n",
    "\n",
    "os_results = validation_results(rf_os_random_model, X_val, y_val, \"Random Forest Random (SMOTE)\", os_results)\n",
    "os_models[\"Random Forest Random (SMOTE)\"] = rf_os_random_model\n",
    "\n",
    "# View best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(\"Random Forest Random (SMOTE)\",X_val, y_val, rf_os_random_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap':[True],\n",
    "    'max_depth': [9, 10, 11], \n",
    "    'max_features': [3, 4, 5, 6],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'min_samples_split': [1, 2, 3, 4], \n",
    "    'n_estimators': [400, 500, 600, 700, 800, 900]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state = seed)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = model_selection.GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit grid to search data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_os_grid_model = grid_search.best_estimator_\n",
    "\n",
    "os_results = validation_results(rf_os_grid_model, X_val, y_val, \"Random Forest Grid (SMOTE)\", os_results)\n",
    "os_models[\"Random Forest Grid (SMOTE)\"] = rf_os_grid_model\n",
    "plot_roc_pr_curves(\"Random Forest Grid (SMOTE)\", X_val, y_val, rf_os_grid_model)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_models_df = pd.DataFrame.from_dict(os_results, orient='index', columns = ['accuracy', 'balanced_accuracy',\n",
    "                                                                               'roc_auc', 'average_precision', \n",
    "                                                                               'f1', 'precision', 'recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_validation_results(os_results, \"SMOTE Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write ensemble models to csv\n",
    "models_df.to_csv(\"models_df.csv\")\n",
    "os_models_df.to_csv(\"os_models_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Model Evaluation - Held Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate held out dataset\n",
    "held_out_df = df_one_hot[df_one_hot.subject_id.isin(held_out_patients)]\n",
    "y_heldout = held_out_df['ln_5years']\n",
    "X_heldout = held_out_df.loc[:, held_out_df.columns != 'ln_5years']\n",
    "X_heldout.drop([ 'subject_id'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_heldout))\n",
    "print(len(y_heldout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name, clf, X_test, y_test, fig_size=(10,8)):\n",
    "    # clf: trained classifier (i.e. after using fit function)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "    # print out stats\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Model accuracy: %.3f\\n\" % accuracy)\n",
    "\n",
    "    # precision, recall, and f1-score is usually reported for class 1 (in binary case)\n",
    "    # recall of positive class (1) = sensitivity\n",
    "    # recall of negative class (0) = specificity\n",
    "    # precision of positive class (1) = PPV\n",
    "    # precision of negative class (0) = NPV\n",
    "    print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm_disp = metrics.plot_confusion_matrix(clf, X_test, y_test)  \n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/' + str(name) + '_cm_test.png')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    auroc = metrics.roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    fig_roc, ax_roc = plt.subplots(1,1,figsize=fig_size)\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_test, y_test, ax=ax_roc)\n",
    "    ax_roc.set_xlabel(\"False Positive Rate\", fontsize = 18)\n",
    "    ax_roc.set_ylabel(\"True Positive Rate\", fontsize = 18)\n",
    "    ax_roc.set_title(\"Receiver Operating Characteristic (ROC) Curve\", fontsize = 18)\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/' + str(name) + '_roc_test.png')\n",
    "    plt.show()\n",
    "\n",
    "    # PR Curve and AP (average precision)\n",
    "    fig, ax_pr = plt.subplots(1,1,figsize=fig_size)\n",
    "    pr_disp = metrics.plot_precision_recall_curve(clf, X_test, y_test, ax=ax_pr)\n",
    "    ax_pr.set_xlabel(\"Recall\", fontsize = 18)\n",
    "    ax_pr.set_ylabel(\"Precision\", fontsize = 18)\n",
    "    ax_pr.set_title(\"Precision-Recall (PR) Curve\", fontsize = 18)\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots2/' + str(name) + '_pr_test.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(\"lr\", lr, X_heldout, y_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both validation and test roc on same plot\n",
    "\n",
    "def plot_both(name, clf, X_val, y_val, X_test, y_test, fig_size=(10,8)):\n",
    "    # clf: trained classifier (i.e. after using fit function)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_pred_proba = clf.predict_proba(X_val)\n",
    "\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    auroc = metrics.roc_auc_score(y_val, y_pred_proba[:, 1])\n",
    "    fig_roc, ax_roc = plt.subplots(1,1,figsize=fig_size)\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_val, y_val, ax=ax_roc, name = \"Internal Validation\")\n",
    "    ax_roc.set_xlabel(\"False Positive Rate\", fontsize = 18)\n",
    "    ax_roc.set_ylabel(\"True Positive Rate\", fontsize = 18)\n",
    "    ax_roc.set_title(\"Receiver Operating Characteristic (ROC) Curve \" + str(name), fontsize = 20)\n",
    "    \n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_proba_test = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    auroc_test = metrics.roc_auc_score(y_test, y_pred_proba_test[:, 1])\n",
    "\n",
    "    roc_disp = metrics.plot_roc_curve(clf, X_test, y_test, ax=ax_roc, name = \"Internal Evaluation on Held-Out Test Set\")\n",
    "    plt.savefig('/Users/katelyn/Documents/Research/plots1/' + str(name) + '_both_roc.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both(\"lr_best_model\", lr_best_model, X_val, y_val, X_heldout, y_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "test_results = validation_results(lr_best_model, X_heldout, y_heldout,\"lr_best_model\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = validation_results(lr_cv_model, X_heldout, y_heldout,\"lr_cv_model\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = validation_results(rf_baseline, X_heldout, y_heldout,\"rf_baseline\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = validation_results(rf_os_random_model, X_heldout, y_heldout, \"rf_os_random_model\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = validation_results(gb_baseline, X_heldout, y_heldout, \"gb_baseline\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num female:\", sum(outcome_yes.gender == 1))\n",
    "print(\"% female:\", sum(outcome_yes.gender == 1)/len(outcome_yes))\n",
    "print(\"Num female outcome no:\", sum(outcome_no.gender == 1))\n",
    "print(\"% female outcome no:\", sum(outcome_no.gender == 1)/len(outcome_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2608-2367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "155-133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_yes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = outcome_yes.groupby(['race_final'],as_index = False).agg({\n",
    "    'subject_id': 'nunique'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race['total'] = 155\n",
    "race['percent'] = race.subject_id/race.total\n",
    "race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race1 = outcome_no.groupby(['race_final'],as_index = False).agg({\n",
    "    'subject_id': 'nunique'\n",
    "})\n",
    "race1['total'] = 2608\n",
    "race1['percent'] = race1.subject_id/race1.total\n",
    "race1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth = outcome_no.groupby(['ethnicity_final'],as_index = False).agg({\n",
    "    'subject_id': 'nunique'\n",
    "})\n",
    "eth['total'] = 2608\n",
    "eth['percent'] = eth.subject_id/eth.total\n",
    "eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth1 = outcome_yes.groupby(['ethnicity_final'],as_index = False).agg({\n",
    "    'subject_id': 'nunique'\n",
    "})\n",
    "eth1['total'] = 155\n",
    "eth1['percent'] = eth1.subject_id/eth1.total\n",
    "eth1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_no.age_at_diagnosis.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_yes.age_at_diagnosis.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
